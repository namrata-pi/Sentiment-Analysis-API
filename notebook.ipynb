{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a508343d-b1f5-4e77-aab0-ca330d0130fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('reply_classification_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "093b5b6d-62a6-456b-a3c3-6e46816c6c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          reply    label\n",
      "1276  Can u provide case study?  neutral\n",
      "673   Can u provide case study?  neutral\n",
      "1540  Can u provide case study?  neutral\n",
      "1108  Can u provide case study?  neutral\n",
      "709   Can u provide case study?  neutral\n",
      "1612  Can u provide case study?  neutral\n",
      "463   Can u provide case study?  neutral\n",
      "745   Can u provide case study?  neutral\n",
      "1144  Can u provide case study?  neutral\n",
      "259   Can u provide case study?  neutral\n",
      "784   Can u provide case study?  neutral\n",
      "1042  Can u provide case study?  neutral\n",
      "1672  Can u provide case study?  neutral\n",
      "403   Can u provide case study?  neutral\n",
      "1756  Can u provide case study?  neutral\n",
      "964   Can u provide case study?  neutral\n",
      "1792  Can u provide case study?  neutral\n",
      "1642  Can u provide case study?  neutral\n",
      "1492  Can u provide case study?  neutral\n",
      "1576  Can u provide case study?  neutral\n"
     ]
    }
   ],
   "source": [
    "conflicts = df[df.duplicated(subset='reply', keep=False)].sort_values('reply')\n",
    "print(conflicts.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2778ac8-c26a-4c51-b957-722b91779133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='reply', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70d87149-4e72-4cc8-9c0a-46f4517e3341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(321, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ed0d8aa-88dd-4198-ba8b-200a73b67e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "neutral     116\n",
      "positive     73\n",
      "POSITIVE     40\n",
      "negative     32\n",
      "NEGATIVE     27\n",
      "Negative     27\n",
      "Neutral       3\n",
      "NEUTRAL       2\n",
      "Positive      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a2c757a-7ace-4e99-aed6-42eb277dcbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "neutral     121\n",
      "positive    114\n",
      "negative     86\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['label'].str.lower()\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71ada658-806b-42a8-96c8-5290cc69990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label  label_encoded\n",
      "0   neutral              0\n",
      "1  positive              1\n",
      "2  negative             -1\n",
      "3   neutral              0\n",
      "4  positive              1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_map = {\n",
    "    \"positive\": 1,\n",
    "    \"neutral\": 0,\n",
    "    \"negative\": -1\n",
    "}\n",
    "\n",
    "\n",
    "df['label_encoded'] = df['label'].map(label_map)\n",
    "\n",
    "print(df[['label', 'label_encoded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be4a7982-a38f-4fc8-bb10-5bf56bfdffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               can we discuss pricing\n",
      "1    im excited to explore this further please send...\n",
      "2                     we not looking for new solutions\n",
      "3                  could you clarify features included\n",
      "4             let us schedule a meeting to dive deeper\n",
      "5                           please remove me from list\n",
      "6                      this looks promising send specs\n",
      "7                      i'll need to check with my team\n",
      "8                   were already using similar product\n",
      "9                              looking forward to demo\n",
      "Name: reply, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_custom(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation.replace(\"?\", \"\").replace(\"!\", \"\")))\n",
    "\n",
    "    text = text.replace(\"?\", \"\").replace(\"!\", \"\")\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    text = re.sub(r'\\bu\\b', 'you', text)\n",
    "    text = re.sub(r'\\bplz\\b', 'please', text)\n",
    "    text = re.sub(r'\\bw/\\b', 'with', text)\n",
    "    text = re.sub(r'\\bw\\b', 'with', text)   \n",
    "\n",
    "    text = re.sub(r\"\\bill\\b\", \"i'll\", text)\n",
    "\n",
    "    text = text.replace('schdule', 'schedule')\n",
    "    text = text.replace('intrsted', 'interested')\n",
    "    text = text.replace('alredy', 'already')\n",
    "    text = text.replace('oppurtunity', 'opportunity')\n",
    "    text = text.replace('intrest', 'interest')\n",
    "    text = text.replace('commited', 'committed')\n",
    "    text = text.replace('lets', 'let us')\n",
    "\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "df['reply'] = df['reply'].apply(clean_text_custom)\n",
    "\n",
    "print(df['reply'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca73b48f-4a07-4028-8b5e-55e85f5dff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"preprocessed2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "426bf35f-e7e8-466f-a9e5-812ad1e3223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model accuracy: 0.9692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.94      0.97        17\n",
      "     neutral       1.00      0.96      0.98        25\n",
      "    positive       0.92      1.00      0.96        23\n",
      "\n",
      "    accuracy                           0.97        65\n",
      "   macro avg       0.97      0.97      0.97        65\n",
      "weighted avg       0.97      0.97      0.97        65\n",
      "\n",
      "Model saved as 'sentiment1_model.pkl'\n",
      "Test prediction: 1, probabilities: [0.13758398 0.12183695 0.74057907]\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "df = pd.read_excel('preprocessed2.xlsx')\n",
    "X = df['reply']\n",
    "y = df['label_encoded']\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000, ngram_range=(1,2), stop_words='english')),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "print(\"Training model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "\n",
    "joblib.dump(pipeline, 'sentiment1_model.pkl')\n",
    "print(\"Model saved as 'sentiment1_model.pkl'\")\n",
    "\n",
    "\n",
    "loaded_model = joblib.load('sentiment1_model.pkl')\n",
    "test_prediction = loaded_model.predict([\"this looks great!\"])[0]\n",
    "test_proba = loaded_model.predict_proba([\"this looks great!\"])[0]\n",
    "print(f\"Test prediction: {test_prediction}, probabilities: {test_proba}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d77ac2e-8329-44bd-8769-419173269770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (321, 3)\n",
      "Class distribution:\n",
      "label\n",
      "neutral     121\n",
      "positive    114\n",
      "negative     86\n",
      "Name: count, dtype: int64\n",
      "Sample texts: ['can we discuss pricing', 'im excited to explore this further please send contract', 'we not looking for new solutions', 'could you clarify features included', 'let us schedule a meeting to dive deeper']\n",
      "Sample labels: [0, 1, -1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#naive bayes \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "df = pd.read_excel('preprocessed2.xlsx')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "X = df['reply'] \n",
    "y = df['label_encoded']  \n",
    "\n",
    "print(f\"Sample texts: {X.head().tolist()}\")\n",
    "print(f\"Sample labels: {y.head().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e166ebd-3bb9-4cb6-a071-5675f264a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    min_df=1,\n",
    "    max_df=0.95,\n",
    "    ngram_range=(1, 2),\n",
    "    lowercase=True,\n",
    "    stop_words='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf9e2610-0ac5-4190-83ac-53818d0fa081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB + TF-IDF:\n",
      "  Mean CV Accuracy: 0.9597 (+/- 0.0868)\n",
      "\n",
      "Best model: Multinomial NB + TF-IDF\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Multinomial NB + TF-IDF': Pipeline([\n",
    "        ('vectorizer', tfidf_vectorizer),\n",
    "        ('classifier', MultinomialNB(alpha=1.0))\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, pipeline in models.items():\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "    results[name] = {\n",
    "        'mean_accuracy': cv_scores.mean(),\n",
    "        'std_accuracy': cv_scores.std(),\n",
    "        'scores': cv_scores\n",
    "    }\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print()\n",
    "\n",
    "\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['mean_accuracy'])\n",
    "print(f\"Best model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecc7d0db-b149-4e0f-a093-ce5249d94fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9077\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        17\n",
      "     neutral       0.96      0.96      0.96        25\n",
      "    positive       0.82      1.00      0.90        23\n",
      "\n",
      "    accuracy                           0.91        65\n",
      "   macro avg       0.93      0.89      0.90        65\n",
      "weighted avg       0.92      0.91      0.90        65\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12  1  4]\n",
      " [ 0 24  1]\n",
      " [ 0  0 23]]\n",
      "\n",
      "Confusion Matrix (Percentages):\n",
      "[[ 70.6   5.9  23.5]\n",
      " [  0.   96.    4. ]\n",
      " [  0.    0.  100. ]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "best_pipeline = models[best_model_name]\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_pred_proba = best_pipeline.predict_proba(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "target_names = ['negative', 'neutral', 'positive']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "print(\"\\nConfusion Matrix (Percentages):\")\n",
    "print(cm_percentage.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28699069-166c-405e-82b9-8bd3fb9192b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\python_practice\\test\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: torch in d:\\python_practice\\test\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in d:\\python_practice\\test\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\python_practice\\test\\lib\\site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python_practice\\test\\lib\\site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python_practice\\test\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python_practice\\test\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python_practice\\test\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\python_practice\\test\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\python_practice\\test\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\python_practice\\test\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\python_practice\\test\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\python_practice\\test\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\python_practice\\test\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\python_practice\\test\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\python_practice\\test\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\python_practice\\test\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python_practice\\test\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python_practice\\test\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python_practice\\test\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python_practice\\test\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python_practice\\test\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python_practice\\test\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python_practice\\test\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The value specified in an AutoRun registry key could not be parsed.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f426be58-e472-4719-b809-63ad6294a979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded:\n",
      "Shape: (321, 3)\n",
      "Label distribution:\n",
      "label\n",
      "neutral     121\n",
      "positive    114\n",
      "negative     86\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train size: 256\n",
      "Validation size: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch 1/3:\n",
      "  Train Loss: 0.9937\n",
      "  Val Loss: 0.8520\n",
      "Epoch 2/3:\n",
      "  Train Loss: 0.6831\n",
      "  Val Loss: 0.5989\n",
      "Epoch 3/3:\n",
      "  Train Loss: 0.5050\n",
      "  Val Loss: 0.5036\n",
      "\n",
      "Final Evaluation:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.94      0.97        17\n",
      "     neutral       0.95      0.84      0.89        25\n",
      "    positive       0.85      1.00      0.92        23\n",
      "\n",
      "    accuracy                           0.92        65\n",
      "   macro avg       0.94      0.93      0.93        65\n",
      "weighted avg       0.93      0.92      0.92        65\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  1  0]\n",
      " [ 0 21  4]\n",
      " [ 0  0 23]]\n",
      "\n",
      "Testing predictions on sample texts:\n",
      "\n",
      "Text: 'im excited explore plz send contract'\n",
      "Prediction: positive (confidence: 0.552)\n",
      "\n",
      "Text: 'not looking new solution'\n",
      "Prediction: negative (confidence: 0.623)\n",
      "\n",
      "Text: 'can discus pricing'\n",
      "Prediction: neutral (confidence: 0.524)\n",
      "\n",
      "Model saved to './customer_sentiment_model'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_excel('preprocessed2.xlsx')\n",
    "\n",
    "label_map = {\n",
    "    \"positive\": 2,    \n",
    "    \"neutral\": 1,     \n",
    "    \"negative\": 0     \n",
    "}\n",
    "\n",
    "df['label_encoded'] = df['label'].map(label_map)\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['reply'].tolist(),\n",
    "    df['label_encoded'].tolist(),  \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label_encoded']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {len(train_texts)}\")\n",
    "print(f\"Validation size: {len(val_texts)}\")\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class CustomerResponseDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = CustomerResponseDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomerResponseDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=3  \n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * 3\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(data_loader), predictions, true_labels\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(3):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    val_loss, val_preds, val_labels = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/3:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "_, predictions, true_labels = evaluate_model(model, val_loader, device)\n",
    "\n",
    "reverse_label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "pred_labels = [reverse_label_map[pred] for pred in predictions]\n",
    "true_label_names = [reverse_label_map[label] for label in true_labels]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_label_names, pred_labels))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(true_label_names, pred_labels))\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = F.softmax(outputs.logits, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return {\n",
    "        'predicted_label': label_map[predicted_class],\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'negative': probabilities[0][0].item(),\n",
    "            'neutral': probabilities[0][1].item(),\n",
    "            'positive': probabilities[0][2].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"\\nTesting predictions on sample texts:\")\n",
    "test_texts = [\n",
    "    \"im excited explore plz send contract\",\n",
    "    \"not looking new solution\",\n",
    "    \"can we discuss pricing\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_sentiment(text, model, tokenizer, device)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Prediction: {result['predicted_label']} (confidence: {result['confidence']:.3f})\")\n",
    "\n",
    "model.save_pretrained('./customer_sentiment_model')\n",
    "tokenizer.save_pretrained('./customer_sentiment_model')\n",
    "print(\"\\nModel saved to './customer_sentiment_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa9e4960-976f-4aac-84e2-85ff5808e8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing predictions on sample texts:\n",
      "\n",
      "Text: 'im excited explore please send contract'\n",
      "Prediction: positive (confidence: 0.573)\n",
      "\n",
      "Text: 'we not looking for new solution'\n",
      "Prediction: negative (confidence: 0.517)\n",
      "\n",
      "Text: 'can we discuss pricing'\n",
      "Prediction: neutral (confidence: 0.417)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting predictions on sample texts:\")\n",
    "test_texts = [\n",
    "    \"im excited explore please send contract\",\n",
    "    \"we not looking for new solution\",\n",
    "    \"can we discuss pricing\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_sentiment(text, model, tokenizer, device)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Prediction: {result['predicted_label']} (confidence: {result['confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7a5fe455-d038-4843-8926-8383364a579f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>96.92%</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DistilBERT</td>\n",
       "      <td>92.31%</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>90.77%</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Accuracy  F1-Score\n",
       "0  Logistic Regression   96.92%      0.97\n",
       "1           DistilBERT   92.31%      0.93\n",
       "2          Naive Bayes   90.77%      0.90"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"Logistic Regression\", \"DistilBERT\", \"Naive Bayes\"],\n",
    "    \"Accuracy\": [\"96.92%\", \"92.31%\", \"90.77%\"],\n",
    "    \"F1-Score\": [0.97, 0.93, 0.90]\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01c62d-8eca-41ec-96c3-689290f0249c",
   "metadata": {},
   "source": [
    "Logistic Regression demonstrated superior performance on this small dataset, achieving a high accuracy of 96%. This model proved to be highly effective for making predictions, unlike Naive Bayes and DistilBERT, which both yielded lower accuracy scores. Their performance could likely be improved with a larger dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e1a8d-ae79-416c-a3bf-52f7611c0c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
